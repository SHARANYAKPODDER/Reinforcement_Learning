{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from gym import Env\n", "from gym.spaces import Discrete, Box, Dict\n", "import numpy as np\n", "from collections import defaultdict\n", "import matplotlib as plt\n", "import plottin\n", "import itertools"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class WarehouseAgent(Env):\n", "    def __init__(self):\n", "        self.GRID_DIM = [7,6]\n", "        self.agent_position = [1,2]\n", "        self.box_location = [4,3]\n", "        self.goal_location = [3,1]\n", "        self._action_to_direction = {\n", "            0: np.array([-1, 0]),\n", "            1: np.array([1, 0]),\n", "            2: np.array([0, -1]),\n", "            3: np.array([0, 1]),\n", "        }\n", "        self._ACTIONLOOKUP = {\n", "            0: 'move up',\n", "            1: 'move down',\n", "            2: 'move left',\n", "            3: 'move right',\n", "            4: 'push'\n", "            }\n", "        self.GRID_DIM = np.asarray(self.GRID_DIM)\n", "        self.GRID = np.zeros(self.GRID_DIM ) # The Boundaries are the walls, so playing space is only [:-2,:-2] \n", "        self.GRID[:,[0,-1]] = 1\n", "        self.GRID[[0,-1],:] = 1\n", "        self.GRID[[1,2,5],3:5] = 1\n", "        self.walls = 1\n", "        self.action_space = Discrete(len(self._ACTIONLOOKUP.keys()))\n", "        self.state_space = Discrete(self.GRID_DIM[0]*self.GRID_DIM[1])\n", "        self.observation_space = Dict(\n", "            {\n", "                \"agent\": Box(np.array([0,0]), np.array([self.GRID_DIM[0]-1,self.GRID_DIM[1] - 1]), shape=(2,), dtype=int),\n", "                'box' : Box( np.array([0,0]), np.array([self.GRID_DIM[0]-1,self.GRID_DIM[1] - 1]), shape=(2,), dtype=int),\n", "                \"target\": Box( np.array([0,0]), np.array([self.GRID_DIM[0]-1,self.GRID_DIM[1] - 1]), shape=(2,), dtype=int),\n", "            })\n", "        self._agent_location = np.array(self.agent_position)\n", "        self._box_location = np.array(self.box_location)\n", "        self._target_location = np.array(self.goal_location) \n", "            \n", "    def step(self, action):\n", "        self._prev_agent_location = None\n", "        self._prev_box_location = None\n", "        moved_box = False\n", "        if action<4:\n", "            moved_player = self._move(action)\n", "        else:\n", "            moved_player, moved_box = self._push(action)\n", "            \n", "        done, reward = self.is_over()            \n", "        observation = self._get_obs()\n", "        info = self._get_info()\n", "        \n", "        return observation, reward, done, info      \n", "        \n", "    def render(self):\n", "        rend = self.GRID.copy().astype(dtype='U1')\n", "        rend[self._agent_location[0],self._agent_location[1]] = 'A'\n", "        rend[self._box_location[0],self._box_location[1]] = 'B'\n", "        rend[self._target_location[0],self._target_location[1]] = 'T'\n", "        return rend\n", "        \n", "    def reset(self,seed = None, return_info = False, options = None):\n", "        self._agent_location = np.array(self.agent_position)\n", "        self._box_location = np.array(self.box_location)\n", "        self._target_location = np.array(self.goal_location)\n", "        \n", "        observation = self._get_obs()\n", "        info = self._get_info()\n", "        return (observation, info) if return_info else observation\n", "        \n", "    def _state_in_seq(self):\n", "        m, n = self._agent_location\n", "        seq = m * self.GRID.shape[1] + n\n", "        return seq\n", "    def _get_obs(self):\n", "        return {\"agent\":self._agent_location,\"box\": self._box_location,\"target\":self._target_location}\n", "    def _get_info(self):\n", "        return {'distance': np.linalg.norm(self._box_location - self._target_location,ord = 1)}\n", "    def _push(self,action):\n", "        loc = self._box_location - self._agent_location\n", "#         print(f'loc{loc}, box :{self._box_location}, agent:{self._agent_location}')\n", "        push_dir = None\n", "        for idx,val in enumerate(self._action_to_direction.values()):\n", "            if np.array_equal(loc,val):\n", "                valid = True\n", "                push_dir = idx\n", "                break\n", "            else :\n", "                valid = False\n", "            \n", "        if valid:\n", "            self._prev_agent_location = self._agent_location\n", "            self._prev_box_location = self._box_location\n", "            self._box_location = self._box_location + self._action_to_direction[push_dir]\n", "            if self.GRID[self._box_location[0],self._box_location[1]] == 1:\n", "                self._box_location = self._prev_box_location\n", "                return False, False\n", "            else:\n", "                self._agent_location = self._agent_location + self._action_to_direction[push_dir]\n", "                return True, True\n", "        \n", "        return False, False\n", "            \n", "    def _move(self,action):\n", "            self._prev_agent_location = self._agent_location\n", "            self._prev_box_location = self._box_location\n", "            self._agent_location = self._agent_location + self._action_to_direction[action]\n", "#             print(self.GRID[self._agent_location],self._agent_location,self.GRID)\n", "            if self.GRID[self._agent_location[0],self._agent_location[1]] == 1:\n", "                self._agent_location = self._prev_agent_location\n", "                return False\n", "            elif np.array_equal(self._agent_location, self._box_location):\n", "                self._agent_location = self._prev_agent_location\n", "                return False\n", "            return True\n", "            \n", "    def is_over(self):\n", "        if np.array_equal(self._box_location, self._target_location):\n", "            done = True\n", "            reward = 0\n", "        elif sum([True if self.GRID[(self._box_location + val)[0],(self._box_location + val)[1]] == 1 else False for val in self._action_to_direction.values()])>1 :\n", "            done = True\n", "            reward = -1\n", "        else: \n", "            done = False\n", "            reward = -1\n", "        return done , reward"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = WarehouseAgent()\n", "env._get_obs()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env._state_in_seq()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%% [markdown]<br>\n", "SARSA"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = WarehouseAgent()\n", "n_states, n_actions = env.state_space.n, env.action_space.n\n", "Q = np.zeros((n_states, n_actions))\n", "def ep_greedy(env,Q,epsilon=0.9):\n", "    seq = env._state_in_seq()\n", "    if np.random.random()<epsilon:\n", "#         print(Q[seq,:])\n", "        x=(Q[seq,:]!=0).all()\n", "#         print(x,'here')\n", "        if x :           \n", "            action = np.argmax(Q[seq,:])\n", "#             print('h')\n", "        else:\n", "            action = np.where(Q[seq,:]==0)[0]\n", "#             print(action)\n", "            action=action[0]\n", "#             print('why')\n", "    else:\n", "        action = np.random.randint(env.action_space.n)\n", "#     print(action)\n", "    return action\n", "def Sarsa(env,alpha, gamma, epsilon, episodes, max_steps):\n", "    timestep_reward = []\n", "    for ep in range(episodes):\n", "        env.reset()\n", "        done = False\n", "        total_reward = 0        \n", "        curr_state = env._state_in_seq()\n", "        curr_a = ep_greedy(env,Q)\n", "        t = 0\n", "        while not done :\n", "            obs, reward, done, info = env.step(curr_a)\n", "            next_state = env._state_in_seq()\n", "            total_reward+= reward\n", "            next_act = ep_greedy(env,Q)\n", "    #         s = curr_state \n", "    #         s_ = next_state \n", "    #         a_ = next_act\n", "            t+=1\n", "            Q[curr_state, curr_a] += alpha * ( reward + (gamma * Q[next_state, next_act] ) - Q[curr_state, curr_a] )\n", "            curr_state = next_state\n", "            curr_a = next_act\n", "    #         print(reward)\n", "#             env.render()\n", "        print(t)\n", "        timestep_reward.append(total_reward)\n", "    return timestep_reward"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["epsilon = 1e-2\n", "epsisodes=100\n", "max_steps = 10\n", "alpha = 0.5\n", "gamma = 0.95"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Sarsa(epsilon=epsilon, alpha=alpha, gamma=gamma, max_steps=max_steps, env=env,episodes=epsisodes)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%% [markdown]<br>\n", "Q-Learning"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n", "\t\"\"\"\n", "\tCreates an epsilon-greedy policy based\n", "\ton a given Q-function and epsilon.\n", "\t\n", "\tReturns a function that takes the state\n", "\tas an input and returns the probabilities\n", "\tfor each action in the form of a numpy array\n", "\tof length of the action space(set of possible actions).\n", "\t\"\"\"\n", "\tdef policyFunction(state):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tAction_probabilities = np.ones(num_actions,\n", "\t\t\t\tdtype = float) * epsilon / num_actions\n", "\t\t\t\t\n", "\t\tbest_action = np.argmax(Q[state])\n", "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n", "\t\treturn Action_probabilities"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\treturn policyFunction"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def qLearning(env, num_episodes=100, discount_factor = 1.0,\n", "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n", "\t\"\"\"\n", "\tQ-Learning algorithm: Off-policy TD control.\n", "\tFinds the optimal greedy policy while improving\n", "\tfollowing an epsilon-greedy policy\"\"\"\n", "\t\n", "\t# Action value function\n", "\t# A nested dictionary that maps\n", "\t# state -> (action -> action-value).\n", "\tQ = defaultdict(lambda: np.zeros(env.action_space.n))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# Keeps track of useful statistics\n", "\tstats = plottin.EpisodeStats(\n", "\t\tepisode_lengths = np.zeros(num_episodes),\n", "\t\tepisode_rewards = np.zeros(num_episodes))\t\n", "\t\n", "\t# Create an epsilon greedy policy function\n", "\t# appropriately for environment action space\n", "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n", "\t\n", "\t# For every episode\n", "\tfor ith_episode in range(num_episodes):\n", "\t\t\n", "\t\t# Reset the environment and pick the first action\n", "\t\treset_state = env.reset()\n", "\t\tstate = env._state_in_seq()\n", "\t\t\n", "\t\tfor t in itertools.count():\n", "\t\t\t\n", "\t\t\t# get probabilities of all actions from current state\n", "\t\t\taction_probabilities = policy(state)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t# choose action according to\n", "\t\t\t# the probability distribution\n", "\t\t\taction = np.random.choice(np.arange(\n", "\t\t\t\t\tlen(action_probabilities)),\n", "\t\t\t\t\tp = action_probabilities)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t# take action and get reward, transit to next state\n", "\t\t\tobs, reward, done, info = env.step(action)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tnext_state = env._state_in_seq()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t# Update statistics\n", "\t\t\tstats.episode_rewards[ith_episode] += reward\n", "\t\t\tstats.episode_lengths[ith_episode] = t\n", "\t\t\t\n", "\t\t\t# TD Update\n", "\t\t\tbest_next_action = np.argmax(Q[next_state])\t\n", "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n", "\t\t\ttd_delta = td_target - Q[state][action]\n", "\t\t\tQ[state][action] += alpha * td_delta"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t# done is True if episode terminated\n", "\t\t\tif done:\n", "\t\t\t\tbreak\n", "\t\t\t\t\n", "\t\t\tstate = next_state\n", "\t\n", "\treturn Q, stats"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = WarehouseAgent()\n", "qLearning(env=env)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}